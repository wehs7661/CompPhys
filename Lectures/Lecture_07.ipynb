{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-100899e0deea9789",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 7: Introduction to Monte Carlo methods\n",
    "\n",
    "## Physics 7810, Spring 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-66620da88b06fd9e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 7.1 - Overview\n",
    "\n",
    "The average of any property $A({\\bf r}^N)$ that depends only on particle coordinates (e.g., the potential energy $U$) is, like the configurational partition function $Z^\\mathrm{int}$, a $3N$-dimensional integral of the form\n",
    "\n",
    "$$\n",
    "\\langle A \\rangle = \\frac{1}{Z^\\mathrm{int}} \\int d{\\bf r}^N A({\\bf r}^N) \\exp \\left[ - \\beta U({\\bf r}^N) \\right].\n",
    "$$\n",
    " \n",
    "Even for relatively small systems (say $N = 100$), brute force evaluation of such integrals using numerical quadrature is infeasible. If we discretize configuration space with $m = 10$ grid points along each of the $3N$ coordinate axes (a very crude approximation), we'd need to evaluate the integrand at $10^{300}$ points!\n",
    " \n",
    "How can we evaluate such high-dimensional integrals?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-66620da88b06fd9e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "First note that the ratio $\\exp[- \\beta U({\\bf r}^N)] / Z^\\mathrm{int}$ is just the probability density to find the system in a configuration near ${\\bf r}^N$,\n",
    "\n",
    "$$\n",
    "\\rho({\\bf r}^N) = \\frac{\\exp[- \\beta U({\\bf r}^N)]}{Z^\\mathrm{int}},\n",
    "$$\n",
    "\n",
    "so the average of a property $A({\\bf r}^N)$ can be written\n",
    "\n",
    "$$\n",
    "\\langle A \\rangle = \\int d{\\bf r}^N A({\\bf r}^N) \\rho({\\bf r}^N).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-66620da88b06fd9e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note that the probability density $\\rho({\\bf r}^N) = \\exp[- \\beta U({\\bf r}^N)] / Z^\\mathrm{int}$ is, in general, a rapidly varying function of ${\\bf r}^N$ in the $3N$-dimensional coordinate space.\n",
    "\n",
    "Also, $\\rho({\\bf r}^N)$ is typically *vanishingly small* for the *overwhelming majority* of points ${\\bf r}^N$.\n",
    "\n",
    "For example, in a fluid of 100 hard spheres near the freezing point, the Boltzmann factor is nonzero for only $1$ out of every $10^{300}$ configurations, as the vast majority of configurations contain at least one pair of overlapping spheres ($U({\\bf r}^N) = \\infty$)\n",
    "\n",
    "Numerical quadrature isn't just *infeasible*; it's also *extraordinarily inefficient*.\n",
    " \n",
    "A better approach is to preferentially sample points in configuration space which have a large statistical weight\n",
    "$\\rho({\\bf r}^N)$.\n",
    " \n",
    "This general approach is called *importance sampling*.\n",
    " \n",
    "In the context of statistical physics, a particularly powerful technique is the Monte Carlo importance sampling method introduced in 1953 by Metropolis et al."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-66620da88b06fd9e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 7.2 - Monte Carlo integration\n",
    "\n",
    "Suppose that we want to numerically evaluate a one-dimensional integral $I$,\n",
    "$$\n",
    "I = \\int_a^b dx\\ f(x).\n",
    "$$\n",
    "\n",
    "We can rewrite this integral as $I = (b - a)\\ \\langle f(x) \\rangle$, where $\\langle f(x) \\rangle$ is the average of $f(x)$ over the interval $[a,b]$.\n",
    "\n",
    "Instead of using conventional quadrature where the integrand is evaluated at predetermined values of $x$, we can use *Monte Carlo integration*, where $f(x)$ is evaluated at a large number $L$ of $x$ values *randomly distributed* over the interval $[a,b]$, and $I$ is approximated as\n",
    "\n",
    "$$\n",
    "I = (b - a)\\ \\langle f(x) \\rangle \\approx (b - a)\\ \\frac{1}{L} \\sum_{i = 1}^L f(x_i).\n",
    "$$\n",
    "\n",
    "This approach can be an effective way of evaluating low-dimensional integrals (e.g., within irregularly-shaped volumes), but is still inefficient for computing high-dimensional integrals when the integrand is negligible for the vast majority of points "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-66620da88b06fd9e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 7.3 - Importance sampling\n",
    "\n",
    "To improve upon simple Monte Carlo integration, we can sample points *nonuniformly* over the interval $[a,b]$, according to some *nonnegative*, *normalized* probability density $w(x)$\n",
    "\n",
    "First, we rewrite the integral $I$ as\n",
    "$$\n",
    "I = \\int_a^b dx\\ w(x) \\frac{f(x)}{w(x)}.\n",
    "$$\n",
    "\n",
    "Now assume that $w(x) = du(x)/dx$, where $u(x)$ is a *monotonically increasing* function of $x$, with $u(a) = 0$ and $u(b) = 1$ (these boundary conditions imply that $w(x)$ is normalized)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-66620da88b06fd9e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Then $I$ can be written\n",
    "\n",
    "$$\n",
    "I = \\int_a^b dx\\ \\frac{du(x)}{dx} \\frac{f(x)}{w(x)} = \\int_{u(a)}^{u(b)} du \\frac{f[x(u)]}{w[x(u)]} = \\int_0^1 du \\frac{f[x(u)]}{w[x(u)]},\n",
    "$$\n",
    "\n",
    "where we have changed the integration variable to $u$, and where $f$ and $w$ now depend implicitly on $u$ through $x(u)$\n",
    "\n",
    "We can now generate $L$ values of $u$ uniformly distributed on the interval $[0,1]$ to obtain the following estimate for $I$:\n",
    "\n",
    "$$\n",
    "I \\approx \\frac{1}{L} \\sum_{i = 1}^L \\frac{f[x(u_i)]}{w[x(u_i)]}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-66620da88b06fd9e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What do we gain from this?\n",
    "\n",
    "The answer depends critically on the choice of $w(x)$, as can be seen by considering the variance \n",
    "$\\sigma_I^2 = \\langle ( I_L - I )^2 \\rangle$ in $I_L$, where $I_L$ is the estimate for the integral over $u$ obtained with $L$ randomly sampled points, and $I = \\langle f / w \\rangle$ is the exact value of the integral. Here the angle brackets represent the true average that would be obtained in the limit $L \\rightarrow \\infty$.\n",
    "\n",
    "The variance can be written\n",
    "$$\n",
    "\\sigma_I^2\n",
    "= \\left\\langle \\left[ \\left( \\frac{1}{L} \\sum_{i = 1}^L \\frac{f[x(u_i)]}{w[x(u_i)]} \\right) - \\left\\langle \\frac{f}{w} \\right\\rangle \\right]^2 \\right\\rangle\n",
    "= \\left\\langle \\left[ \\frac{1}{L} \\sum_{i = 1}^L \\left( \\frac{f[x(u_i)]}{w[x(u_i)]} - \\left\\langle \\frac{f}{w} \\right\\rangle \\right) \\right]^2 \\right\\rangle\n",
    "$$\n",
    "$$\n",
    "= \\left\\langle \n",
    "\\frac{1}{L^2} \\sum_{i = 1}^L \\sum_{j = 1}^L \n",
    "\\left( \\frac{f[x(u_i)]}{w[x(u_i)]} - \\left\\langle \\frac{f}{w} \\right\\rangle \\right) \n",
    "\\left( \\frac{f[x(u_j)]}{w[x(u_j)]} - \\left\\langle \\frac{f}{w} \\right\\rangle \\right) \n",
    "\\right\\rangle\n",
    "$$\n",
    "$$\n",
    "= \\frac{1}{L^2} \\sum_{i = 1}^L \\sum_{j = 1}^L \\left\\langle \n",
    "\\left( \\frac{f[x(u_i)]}{w[x(u_i)]} - \\left\\langle \\frac{f}{w} \\right\\rangle \\right) \n",
    "\\left( \\frac{f[x(u_j)]}{w[x(u_j)]} - \\left\\langle \\frac{f}{w} \\right\\rangle \\right) \n",
    "\\right\\rangle.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-66620da88b06fd9e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Distinct samples $i$ and $j$ are assumed to be independent, so the cross terms vanish, giving\n",
    "\n",
    "$$\n",
    "\\sigma_I^2 = \\frac{1}{L^2} \\sum_{i = 1}^L\n",
    "\\left\\langle\n",
    "\\left( \\frac{f[x(u_i)]}{w[x(u_i)]} - \\left\\langle \\frac{f}{w} \\right\\rangle \\right)^2 \n",
    "\\right\\rangle\n",
    "= \\frac{1}{L} \\left[\n",
    "\\left\\langle \\left( \\frac{f}{w} \\right)^2 \\right\\rangle - \\left\\langle \\frac{f}{w} \\right\\rangle^2 \n",
    "\\right]\n",
    "$$\n",
    "\n",
    "$\\sigma_I^2 \\sim 1/L$ for any choice of $w$, but the variance can be reduced by choosing $w(x)$ such that the variance in $f(x)/w(x)$ is small.\n",
    "\n",
    "Ideally, we'd choose $f(x) / w(x)$ to be constant, in which case the variance would vanish.\n",
    "\n",
    "If $w(x)$ is constant, we recover brute force Monte Carlo integration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-66620da88b06fd9e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 7.4 - Metropolis Monte Carlo\n",
    " \n",
    "Let's return to configuration-space integrals of the form\n",
    "\n",
    "$$\n",
    "\\langle A \\rangle = \\int d{\\bf r}^N A({\\bf r}^N) \\rho({\\bf r}^N),\n",
    "$$\n",
    "\n",
    "where the probability density $\\rho({\\bf r}^N) = \\exp[- \\beta U({\\bf r}^N)] / Z^\\mathrm{int}$.\n",
    "\n",
    "Can we use simple importance sampling to evaluate such integrals?\n",
    "\n",
    "Unfortunately not, because we don't know how to construct a transformation that would enable us to generate points in configuration space with a *normalized* probability density proportional to the Boltzmann factor.\n",
    "\n",
    "A necessary (but not sufficient) condition is that we must be able to compute the partition function (normalization factor) $Z^\\mathrm{int}$ analytically.\n",
    "\n",
    "If we could do this for a given system of interest, there'd be no need for computer simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-66620da88b06fd9e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Suppose we were somehow able to randomly generate points in configuration space consistent with the probability distribution $\\rho({\\bf r}^N)$.\n",
    "\n",
    "In this case, our estimate of $\\langle A \\rangle$ is just a simple average of the values of $A$ evaluated at the generated points:\n",
    "$$\n",
    "\\langle A \\rangle \\approx \\frac{1}{L} \\sum_{i = 1}^L A({\\bf r}_i^N).\n",
    "$$\n",
    "\n",
    "How does this differ from simple importance sampling?\n",
    "\n",
    "In simple importance sampling, we need to know *a priori* the *absolute* probability of sampling a point in a (hyper)volume $d{\\bf r}^N$ around ${\\bf r}^N$. In other words, we need to know *both* $\\exp[-\\beta U({\\bf r}^N)]$ and $Z^\\mathrm{int}$.\n",
    "\n",
    "In contrast, to calculate $\\langle A \\rangle$ as outlined above, we need to know only the *relative* (not the *absolute*) probability of visiting different points in configuration space. In other words, we only need to know $\\exp[-\\beta U({\\bf r}^N)]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-66620da88b06fd9e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Measuring the depth of the Nile: a comparison of conventional quadrature (left) with the Metropolis scheme (right):\n",
    " \n",
    "<img src=\"images/Frenkel_Fig_3_1.png\" alt=\"Drawing\" style=\"width: 800px;\">\n",
    "\n",
    "Figure from *Understanding Molecular Simulation: from Algorithms to Applications*, by Daan Frenkel and Berend Smit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-66620da88b06fd9e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the Metropolis scheme, a random walk is executed through the region of space where the integrand is nonnegligible (i.e., within the Nile itself).\n",
    "\n",
    "In this random walk, a trial move is rejected if it takes you out of the water and is accepted otherwise. After *every* trial move (accepted or not), the depth is measured, and the (unweighted) average of all measurements is an estimate of the average depth.\n",
    "\n",
    "In the Metropolis importance sampling scheme, the total *area* of the Nile *cannot* be measured, as this quantity is analogous to $Z$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-66620da88b06fd9e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "*How do we generate points in configuration space with a relative probability proportional to the Boltzmann factor?*\n",
    " \n",
    "The basic approach is to first prepare the system in a configuration ${\\bf r}^N$, denoted by $o$ (old), with a non-vanishing Boltzmann factor $\\exp[- \\beta U(o)]$ (e.g., a regular crystalline lattice with no hard-core overlaps).\n",
    " \n",
    "Next, we generate a new trial configuration ${{\\bf r}^\\prime}^N$, denoted by $n$ (new), say by adding a small random displacement $\\delta {\\bf r}^N$ to ${\\bf r}^N$.\n",
    " \n",
    "The Boltzmann factor of this trial configuration is $\\exp[- \\beta U(n)]$.\n",
    " \n",
    "*How do we decide whether to accept or reject this new configuration?*\n",
    " \n",
    "There are many possible rules for making this decision. The only constraint is that, *on average, the probability of finding the system in a configuration $n$ should be proportional to $\\rho(n) = \\exp[- \\beta U(n)] / Z^\\mathrm{int}$*.\n",
    " \n",
    "Here I'll discuss the *Metropolis* algorithm, which is one of the simplest and most generally applicable methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-66620da88b06fd9e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Monte Carlo simulation schemes are defined by the *transition probability* $\\pi(o \\to n)$ to go from configuration $o$ to $n$.\n",
    " \n",
    "To sample points in configuration space with a relative probability given by the Boltzmann factor, the equilibrium probability distribution $\\rho(o)$ must be stationary under Monte Carlo (Markov chain) dynamics.\n",
    " \n",
    "This implies that, in equilibrium, the average number of accepted trial moves that result in the system leaving state $o$ must be exactly equal to the number of accepted trial moves from all other states $n$ to $o$ (*balance* condition):\n",
    "\n",
    "$$\n",
    "\\sum_{n \\neq o} \\rho(o) \\pi(o \\to n) = \\sum_{n \\neq o} \\rho(n) \\pi(n \\to o).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-66620da88b06fd9e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It's convenient to impose a much stronger condition, namely that in equilibrium the average number of accepted moves from $o$ to any other *specific* state $n$ is exactly balanced by the number of reverse moves,\n",
    "\n",
    "$$\n",
    "\\rho(o) \\pi(o \\to n) = \\rho(n) \\pi(n \\to o).\n",
    "$$\n",
    "\n",
    "This is known as the *detailed balance* condition.\n",
    "\n",
    "Many possible forms of transition matrix $\\pi(o \\to n)$ satisfy the detailed balance condition.\n",
    "\n",
    "The total transition matrix is usually written as a product of a *proposal probability* $\\alpha(o \\to n)$ (also called the *underlying matrix* of the Markov chain) and an *acceptance probability* $P_\\mathrm{acc}(o \\to n)$:\n",
    "\n",
    "$$\n",
    "\\pi(o \\to n) = \\alpha(o \\to n) P_\\mathrm{acc}(o \\to n).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-66620da88b06fd9e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the original Metropolis method, $\\alpha$ is chosen to be a *symmetric matrix*,\n",
    "\n",
    "$$\n",
    "\\alpha(o \\to n) = \\alpha(n \\to o),\n",
    "$$\n",
    "\n",
    "so the probability of proposing a trial move from $o$ to $n$ is equal to the probability of proposing the reverse move.\n",
    " \n",
    "This implies that the acceptance probability $P_\\mathrm{acc}$ *isn't* symmetric.\n",
    "\n",
    "In particular, if $\\alpha$ is symmetric, then the detailed balance condition requires\n",
    "\n",
    "$$\n",
    "\\rho(o) P_\\mathrm{acc}(o \\to n) = \\rho(n) P_\\mathrm{acc}(n \\to o),\n",
    "$$\n",
    "\n",
    "and therefore\n",
    "\n",
    "$$\n",
    "\\frac{P_\\mathrm{acc}(o \\to n)}{P_\\mathrm{acc}(n \\to o)} = \\frac{\\rho(n)}{\\rho(o)}\n",
    "= e^{- \\beta \\left[ U(n) - U(o) \\right]} = e^{- \\beta \\Delta U},\n",
    "$$\n",
    "\n",
    "where $\\Delta U = U(n) - U(o)$ is the change in potential energy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-66620da88b06fd9e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Many choices of $P_\\mathrm{acc}$ satisfy this condition, but Metropolis *et al.* chose\n",
    "\n",
    "$$\n",
    "P_\\mathrm{acc}(o \\to n) =\n",
    "\\begin{cases}\n",
    "    e^{- \\beta \\Delta U},& \\text{if } \\Delta U > 0\\\\\n",
    "    1,              & \\text{if } \\Delta U \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "which can be written more compactly as\n",
    "\n",
    "$$\n",
    "P_\\mathrm{acc}(o \\to n) = \\min \\left[1, e^{- \\beta \\Delta U} \\right].\n",
    "$$\n",
    "\n",
    "It's also important to allow for the possibility that the system remains in the same state, so overall we have\n",
    "\n",
    "$$\n",
    "\\pi(o \\to n) = \\alpha(o \\to n) \\min \\left[1, e^{- \\beta \\Delta U} \\right], \\quad n \\neq o\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\pi(o \\to o) = 1 - \\sum_{n \\neq o} \\pi(o \\to n).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-66620da88b06fd9e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note that the acceptance probability only depends on the ratio of probability densities $\\rho(n)/\\rho(o)$, and is therefore independent of $Z^\\mathrm{int}$.\n",
    "\n",
    "We haven't yet specified the matrix $\\alpha$, except to say that it's symmetric.\n",
    " \n",
    "This gives us considerable freedom in our choice of trial moves, and $\\alpha$ can be varied to maximize the Monte Carlo sampling efficiency.\n",
    " \n",
    "It's also essential that the Markov chain samples a representative portion of configuration space in a reasonable number of moves (ergodicity). In practice, one often encounters *bottlenecks* (kinetic barriers) that prevent the system from finding a path between two allowed regions of configuration space, for example in simulations of two-phase coexistence.\n",
    "\n",
    "*Enhanced sampling methods* are sometimes effective in overcoming such bottlenecks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
